{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438508cd-9387-456a-891c-7224a4c73dfd",
   "metadata": {},
   "source": [
    "# Test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64493b59-3738-4e75-8bf9-70de639dbfdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/kimshome/anaconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/kimshome/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Users/kimshome/anaconda3/lib/python3.11/site-packages (from xgboost) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1f0be3-b5ec-42a4-8a07-b23f5dabaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bffdeb-27c6-4245-9888-58917dc22cb0",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "The breast cancer dataset is a classic binary classification dataset in the sklearn dataset library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97822273-1519-4823-90f8-27acab72e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.45997\n",
      "[1]\tvalidation_0-logloss:0.34184\n",
      "[2]\tvalidation_0-logloss:0.27076\n",
      "[3]\tvalidation_0-logloss:0.22399\n",
      "[4]\tvalidation_0-logloss:0.19346\n",
      "[5]\tvalidation_0-logloss:0.16814\n",
      "[6]\tvalidation_0-logloss:0.15393\n",
      "[7]\tvalidation_0-logloss:0.14081\n",
      "[8]\tvalidation_0-logloss:0.13269\n",
      "[9]\tvalidation_0-logloss:0.12515\n",
      "[10]\tvalidation_0-logloss:0.11551\n",
      "[11]\tvalidation_0-logloss:0.11184\n",
      "[12]\tvalidation_0-logloss:0.10799\n",
      "[13]\tvalidation_0-logloss:0.10541\n",
      "[14]\tvalidation_0-logloss:0.10493\n",
      "[15]\tvalidation_0-logloss:0.10326\n",
      "[16]\tvalidation_0-logloss:0.10300\n",
      "[17]\tvalidation_0-logloss:0.10339\n",
      "[18]\tvalidation_0-logloss:0.10160\n",
      "[19]\tvalidation_0-logloss:0.09892\n",
      "[20]\tvalidation_0-logloss:0.09478\n",
      "[21]\tvalidation_0-logloss:0.09359\n",
      "[22]\tvalidation_0-logloss:0.09247\n",
      "[23]\tvalidation_0-logloss:0.09284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=94)\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2)\n",
    "# Fit the model, test sets are used for early stopping.\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "# Save model into JSON format.\n",
    "clf.save_model(\"clf.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a48fd-ff8c-47f5-9323-4a76f7eb6dbe",
   "metadata": {},
   "source": [
    "# Binary Classification \n",
    "The cleveland heart disease dataset is a binary classification dataset found on xgboosting.com. It has more features than the above dataset and uses a grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1a95ba-3f6b-4c6b-b186-5a5c65f1303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (303, 13)\n",
      "Features: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "Class distributions: Counter({1: 165, 0: 138})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimshome/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.839\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the Cleveland Heart Disease dataset (classification dataset)\n",
    "X, y = fetch_openml(\"heart-disease\", return_X_y=True, target_column='target', as_frame=True)\n",
    "\n",
    "# Mark missing as nan\n",
    "X = X.fillna(value=np.nan)\n",
    "\n",
    "# Convert target to integers\n",
    "y = y.astype('int')\n",
    "\n",
    "# Print key information about the dataset\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")\n",
    "print(f\"Class distributions: {Counter(y)}\")\n",
    "\n",
    "# Retrieve values\n",
    "X = X.values\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBClassifier\n",
    "model = XGBClassifier(objective='binary:logistic', random_state=42, n_jobs=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best score and parameters\n",
    "print(f\"Best score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Access best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "best_model.save_model('best_model_heart_disease.ubj')\n",
    "\n",
    "# Load saved model\n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('best_model_heart_disease.ubj')\n",
    "\n",
    "# Use loaded model for predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "accuracy = loaded_model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d15268-30f3-4a3d-b510-e8bcf0f4a280",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "The fetch_covtype dataset (found on xgboosting.com) contains data on forest cover types from four wilderness areas in the Roosevelt National Forest of northern Colorado. Useful example in case we go beyond binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b1ffac-8019-4297-8394-2fe3ebc8353f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (581012, 54)\n",
      "Classes: [0 1 2 3 4 5 6]\n",
      "Class Distributions: Counter({1: 283301, 0: 211840, 2: 35754, 6: 20510, 5: 17367, 4: 9493, 3: 2747})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimshome/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[1;32m     31\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Print best score and parameters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "# Load the Covertype dataset\n",
    "covtype = fetch_covtype()\n",
    "X, y = covtype.data, covtype.target\n",
    "\n",
    "# Ensure class numbers start at 0\n",
    "y = y - 1\n",
    "\n",
    "# Print key information about the dataset\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class Distributions: {Counter(y)}\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBClassifier\n",
    "model = XGBClassifier(objective='multi:softmax', random_state=42, n_jobs=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best score and parameters\n",
    "print(f\"Best score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Access best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "best_model.save_model('best_model_covtype.ubj')\n",
    "\n",
    "# Load saved model\n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('best_model_covtype.ubj')\n",
    "\n",
    "# Use loaded model for predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "accuracy = loaded_model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5328d80-c5db-4a03-b46d-f955a3cd861d",
   "metadata": {},
   "source": [
    "# Creating a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e18347-3949-4029-bd47-949333087ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9866666666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       190\n",
      "           1       0.99      0.97      0.98       110\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.98      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a toy dataset\n",
    "n_samples = 1000  # number of objects\n",
    "n_features = 5   # H(mag), e, a, Peri., Incl.\n",
    "\n",
    "# Generate dataset with make_classification\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples, \n",
    "    n_features=n_features,\n",
    "    n_informative=5,      # all 5 features are informative\n",
    "    n_redundant=0,        # no redundant features\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.6, 0.4],   # balance between NEO (1) and non-NEO (0)\n",
    "    class_sep=1.5,        # separation between classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for feature naming\n",
    "df = pd.DataFrame(X, columns=['H', 'e', 'a', 'Peri.', 'Incl.'])\n",
    "df['label'] = y\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['H', 'e', 'a', 'Peri.', 'Incl.']], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af003cc-d446-46b1-922e-6e3c943f7422",
   "metadata": {},
   "source": [
    "# Add random noise and set ranges for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd806ac-7892-47c2-ba6e-46e8606e045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4766666666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.53      0.50       146\n",
      "           1       0.49      0.42      0.45       154\n",
      "\n",
      "    accuracy                           0.48       300\n",
      "   macro avg       0.48      0.48      0.48       300\n",
      "weighted avg       0.48      0.48      0.48       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimshome/anaconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:37:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Parameters \n",
    "n_samples = 1000\n",
    "H_range = (15, 25)         # Typical range for Absolute Magnitude (H)\n",
    "e_range = (0.0, 0.9)       # Typical range for Eccentricity (e)\n",
    "a_range = (0.5, 5.0)       # Typical range for Semi-major Axis (a), in AU\n",
    "peri_range = (0.3, 2.0)    # Typical range for Perihelion Distance (Peri.), in AU\n",
    "incl_range = (0.0, 30.0)   # Typical range for Inclination (Incl.), in degrees\n",
    "\n",
    "# Generate random values within these ranges\n",
    "np.random.seed(42)\n",
    "H = np.random.uniform(*H_range, n_samples)\n",
    "e = np.random.uniform(*e_range, n_samples)\n",
    "a = np.random.uniform(*a_range, n_samples)\n",
    "peri = np.random.uniform(*peri_range, n_samples)\n",
    "incl = np.random.uniform(*incl_range, n_samples)\n",
    "\n",
    "# Add Gaussian noise to simulate observational noise\n",
    "noise_level = 0.05  # Adjust noise level as needed\n",
    "H += np.random.normal(0, noise_level, n_samples)\n",
    "e += np.random.normal(0, noise_level, n_samples)\n",
    "a += np.random.normal(0, noise_level, n_samples)\n",
    "peri += np.random.normal(0, noise_level, n_samples)\n",
    "incl += np.random.normal(0, noise_level, n_samples)\n",
    "\n",
    "# Create DataFrame and labels\n",
    "df = pd.DataFrame({\n",
    "    'H': H,\n",
    "    'e': e,\n",
    "    'a': a,\n",
    "    'Peri.': peri,\n",
    "    'Incl.': incl\n",
    "})\n",
    "\n",
    "# Assign labels: 1 for NEO and 0 for non-NEO, with a 50-50 distribution\n",
    "labels = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])\n",
    "df['label'] = labels\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['H', 'e', 'a', 'Peri.', 'Incl.']], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48dc17-8393-48ea-a0e3-732a47f6b22f",
   "metadata": {},
   "source": [
    "# Add hyperparameter tuning using a grid search to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f8411e-8838-47cf-ad01-98c9aafe0fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "Best Parameters: {'colsample_bytree': 0.6, 'gamma': 0.1, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "Best Cross-Validation Accuracy: 0.5371428571428571\n",
      "Test Accuracy: 0.4766666666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.54      0.50       146\n",
      "           1       0.49      0.42      0.45       154\n",
      "\n",
      "    accuracy                           0.48       300\n",
      "   macro avg       0.48      0.48      0.48       300\n",
      "weighted avg       0.48      0.48      0.47       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['H', 'e', 'a', 'Peri.', 'Incl.']], df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],         # Controls the impact of each tree\n",
    "    'max_depth': [3, 5, 7],                    # Limits the depth of each tree (to control overfitting)\n",
    "    'n_estimators': [50, 100, 200],            # Number of trees (iterations)\n",
    "    'subsample': [0.7, 0.8, 1.0],              # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],       # Fraction of features used per tree\n",
    "    'gamma': [0, 0.1, 0.2]                     # Minimum loss reduction to split (helps control overfitting)\n",
    "}\n",
    "\n",
    "# Initialize\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    verbose=1,              # Print progress\n",
    "    n_jobs=-1               # Use all available cores\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for final predictions\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d783167f-190a-406c-a2bd-9895775845dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             H         e         a     Peri.      Incl.  label\n",
      "0    18.715552  0.170761  1.677731  1.508594  17.087680      0\n",
      "1    24.387628  0.451302  1.534183  1.702374  24.185213      0\n",
      "2    22.299328  0.873058  4.602790  0.636086  22.825406      1\n",
      "3    21.032259  0.711005  1.664638  1.416443   4.569927      0\n",
      "4    16.587068  0.688916  1.693717  1.180042   4.533532      0\n",
      "..         ...       ...       ...       ...        ...    ...\n",
      "995  15.977320  0.512229  4.370838  1.172521  27.033966      0\n",
      "996  24.222270  0.923181  1.186248  0.943255   3.605252      0\n",
      "997  16.298183  0.173637  1.828990  0.956542   9.827622      1\n",
      "998  24.531737 -0.014695  1.783756  1.102295  24.543122      0\n",
      "999  19.449776  0.292538  4.469602  0.588726  18.011462      1\n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
